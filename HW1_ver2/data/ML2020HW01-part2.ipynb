{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Home Assignment No. 1: Part 2 (Theory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the homework you are to solve several simple theoretical problems related to machine learning algorithms.\n",
    "* For every separate problem you can get only 0 points or maximal points for this problem. There are **NO INTERMEDIATE scores**.\n",
    "* Your solution must me **COMPLETE**, i.e. contain all required formulas/proofs/detailed explanations.\n",
    "* You must write your solution for any problem right after the words **YOUR SOLUTION**. Attaching pictures of your handwriting is allowed, but **highly discouraged**.\n",
    "## $\\LaTeX$ in Jupyter\n",
    "Jupyter has constantly improving $\\LaTeX$ support. Below are the basic methods to\n",
    "write **neat, tidy, and well typeset** equations in your notebooks:\n",
    "* to write an **inline** equation use \n",
    "```markdown\n",
    "$ you latex equation here $\n",
    "```\n",
    "* to write an equation, that is **displayed on a separate line** use \n",
    "```markdown\n",
    "$$ you latex equation here $$\n",
    "```\n",
    "* to write a **block of equations** use \n",
    "```markdown\n",
    "\\begin{align}\n",
    "    left-hand-side\n",
    "        &= right-hand-side on line 1\n",
    "        \\\\\n",
    "        &= right-hand-side on line 2\n",
    "        \\\\\n",
    "        &= right-hand-side on the last line\n",
    "\\end{align}\n",
    "```\n",
    "The **ampersand** (`&`) aligns the equations horizontally and the **double backslash**\n",
    "(`\\\\`) creates a new line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1. Linear Regression (1 point)\n",
    "Let us consider the problem of linear regression for 2D data $(x_{1},y_{1}),\\dots,(x_{n},y_{n})\\in\\mathbb{R}^{2+ 1}$. Let us have $l_{\\infty}$ regularization penalty, i.e. the optimization problem is\n",
    "$$\n",
    "||Xw - y||_2^2 + \\lambda||w||_{\\infty} \\rightarrow \\min_{\\boldsymbol{w}}\n",
    "$$\n",
    "\n",
    "Show that this problem is equal to Lasso regression problem with feature matrix $Z = XA \\in \\mathbb{R}^{n \\times 2}$ for a certain $2 \\times 2$ matrix $A$ and the same target $y$.\n",
    "### Your solution:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For $l_{\\infty}$:\n",
    "\n",
    "$$\\nabla\t F(w)= 2 X^{T}(Xw - Y)+\\lambda sign(w) \\delta_{ij}=0$$\n",
    "\n",
    "$$X^{T}(Xw - Y)+\\frac{\\lambda}{2} sign(w) \\delta_{ij}=0$$\n",
    "\n",
    "$$X^{T}Xw-X^{T}y + \\frac{\\lambda}{2}sign(w) \\delta_{ij}=0$$\n",
    "\n",
    "$$X^{T}Xw=X^{T}y = \\frac{\\lambda}{2}sign(w)\\delta_{ij}$$\n",
    "\n",
    "$$w = (X^{T}X)^{-1}X^{T}y - (X^{T}X)^{-1}\\frac{\\lambda}{2}sign(w)\\delta_{ij} = X^{-1}X^{-T}X^{T}y - X^{-1}X^{-T} \\frac{\\lambda}{2}sign(w)\\delta_{ij}$$\n",
    "\n",
    "$$w = X^{-1}(y- X^{-T} \\frac{\\lambda}{2}sign(w)\\delta_{ij}) \\qquad (1)$$\n",
    "\n",
    "- For $l_{1}$:\n",
    "$$\\nabla\t F(w)= 2 Z^{T}(Zw - Y)+\\lambda sign(w)=0$$\n",
    "$$Z^{T}Zw-Z^{T}y + \\frac{\\lambda}{2}sign(w)=0$$\n",
    "$$w = Z^{-1}Z^{-T}Z^{T}y+(Z^{T}Z)^{-1} \\frac{\\lambda}{2}sign(w)$$\n",
    "$$w = Z^{-1}y+Z^{-1}Z^{-T}\\frac{\\lambda}{2}sign(w)=Z^{-1}(y-Z^{-T}\\frac{\\lambda}{2}sign(w))$$\n",
    "$$w = Z^{-1}(y-Z^{-T}\\frac{\\lambda}{2}sign(w))=A^{-1}X^{-1}(y-(XA)^{-T}\\frac{\\lambda}{2}sign(w)) $$\n",
    "$$w = (XA)^{-1}(y-(XA)^{-T}\\frac{\\lambda}{2}sign(w)) \\qquad (2)$$\n",
    "\n",
    "\n",
    "- By looking at (1) and (2) we can see that they are the same with the only difference represented by matrix $A$. Knowing graphical representation of $l_{\\infty}$ and $l_{1}$ norms it is easy to understand that matrix $A$ is a $45^{o}$ degrees rotation matrix. \n",
    "$$A = 0.5\\begin{bmatrix}\n",
    "\\sqrt2 & -\\sqrt2 \\\\\n",
    "\\sqrt2 & \\sqrt2 \\\\\n",
    "\\end{bmatrix}$$\n",
    "![title](asdd.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2. Probit Regression (1 point)\n",
    "Let us consider the Probit regression model for a binary classification problem. It is a linear model analogous to the Logistic Regression. For a feature vector $x \\in \\mathbb{R}^d$ the probability for label $y$ being equal to 1 is given by\n",
    "$$P(y=1|x) = \\Phi(x^Tw + b).$$ \n",
    "Here $\\Phi(x)=\\int_{-\\infty}^{x}\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{t^{2}}{2}}dt$ is the Cumulative Density Function for the **standard normal distribution**; values $w \\in \\mathbb{R}^d$ and $b \\in \\mathbb{R}$ are learnable parameters of the probit regression model.\n",
    "\n",
    "Write down the optimization problem for training probit  probit regression **without L2-regularization** and show that it does not have optimum in the case when the training set is **lineary separable**.\n",
    "\n",
    "### Your solution:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Log-Likelihood:\n",
    "$$LL = \\sum_{i} [y_{i}log(Ф(x^{T}w+b))+(1-y_{i})log(1 - Ф(x^{T}w+b)]$$\n",
    "$$\\nabla LL = \\sum_{i} [y_{i} \\frac{f(x^{t}w+b)}{Ф(x^{T}w+b)} + (1-y_{i})\\frac{-f(x^Tw+b)}{1-Ф(x^{T}w+b)}$$ where\n",
    "$f(x) = \\sqrt{2\\pi}\\exp{\\frac{x^2}{2}}$\n",
    "    \n",
    "$$LL \\to \\max_{w}$$\n",
    "- $LL$ is concave $\\Longrightarrow\t$\n",
    "\n",
    "$$\\nabla LL = \\sum_{i} [y_{i} \\frac{f(x^{t}w+b)}{Ф(x^{T}w+b)} + (1-y_{i})\\frac{-f(x^Tw+b)}{1-Ф(x^{T}w+b)}] = 0$$\n",
    "\n",
    "$$\\nabla LL = \\sum_{i} [\\frac{f(x^{T}w+b)}{Ф(x^{T}w+b)(1-Ф(x^{T}w+b)}(y_{i}-Ф(x^{T}w+b))x_{i}] = 0$$\n",
    "\n",
    "- This holds when either $f(x^{T}w+b)=0$ or $Ф(x^{T}w+b)(1-Ф(x^{T}w+b)=\\infty $\n",
    "- $Ф(x^{T}w+b)\\in[-1,1]$ meaning that we can achieve the condition only if $f(x^{T}w+b)=0$. This holds if $x^{T}w+b \\to -\\infty$, meaning that there is no optimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3. Multiclass Bayesian Naive Classifier (1 point)\n",
    "Let us consider multiclass classification problem with classes $C_1, \\ldots, C_K$. Assume that all $d$ features $x_1, \\dots,x_d$ are binary, i.e. $x_{i}\\in\\{0,1\\}$ for $i=1,2\\dots,d$. Show that the decision rule of a Bayesian Naive Classifier can be represented as $\\arg \\max$ of linear functions of the input. \n",
    "### Your solution:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bayesian Naive Classifier conditional probability: \n",
    "\n",
    "$$ P(C_k | x) = \\frac{p(C_k)p(x|C_K)}{p(x)} \\sim P(C_k) \\cdot P(x | C_k)$$ \n",
    "\n",
    "In Logarithmic form \n",
    "$$\\log{P(C_k | x)} = \\log{P(C_k)}+\\log{P(C_k | x)}$$\n",
    "\n",
    "- Bernoulli NB form(from the lecture): \n",
    "$$P(x|C_k) = \\Pi_i p_{k_i}^{x_i}(1 - p_{k_i})^{(1 - x_i)} \\implies$$\n",
    "$$\\log{P(C_k | x)} = \\sum_i x_i \\log{p_{k_i}} + \\sum_i (1 - x_i) \\log{(1 - p_{k_i})} + \\log{P(C_k)} = \\sum_i [ x_i \\log{p_{k_i}} - x_i \\log{(1 - p_{k_i})} ] + \\sum_i \\log{(1 - p_{k_i})} + \\log{P(C_k)}=\n",
    "\\sum_i x_i[\\log{p_{k_i}} - x_i \\log{(1 - p_{k_i})} ] + \\sum_i \\log{(1 - p_{k_i})} + \\log{P(C_k)}$$\n",
    "- We can rewrite this in matrix form:\n",
    "$$\n",
    "\\underbrace{\n",
    "\\begin{bmatrix}\n",
    "    \\log{p_{k_1}} - \\log{(1 - p_{k_1})} \\\\\n",
    "    \\vdots \\\\\n",
    "    \\log{p_{k_n}} - \\log{(1 - p_{k_n})}\n",
    "\\end{bmatrix}^\\top}_{W_k^\\top} \\times\n",
    "\\underbrace{\n",
    "\\begin{bmatrix}\n",
    "    x_1 \\\\\n",
    "    \\vdots \\\\\n",
    "    x_n\n",
    "\\end{bmatrix}\n",
    "}_{x}\n",
    "+\n",
    "\\underbrace{\n",
    "\\begin{bmatrix}\n",
    "    \\log{(1 - p_{k_1})} \\\\\n",
    "    \\vdots \\\\\n",
    "    \\log{(1 - p_{k_n})}\n",
    "\\end{bmatrix}\n",
    "+ \\log{P(C_k)}\n",
    "}_{b_k}\n",
    "= W_k^\\top x + b_k\n",
    "$$\n",
    "\n",
    "- Thus we get the decision rule:\n",
    "$$\n",
    "\\hat{y} = \\arg \\max_{k} P(C_k | x) = \\arg \\max_{k} (\\log{P(C_k | x)}) = \\arg \\max_{k} (W_k^\\top x + b_k )\n",
    "$$\n",
    "which is $\\arg \\max$ of linear functions of the input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4. Nearest Neighbors (1 point)\n",
    "Consider the 1-nearest-neighbor classifier applied to multiclass classification problem. Let's denote the classifier fitted on data $X$ by $f_X(\\cdot)$.\n",
    "\n",
    "The formula for complete **leave-k-out cross-validation** on data sample $X^{n}$ is defined as\n",
    "$$L_{k}OCV=\\sum\\limits_{X\\subset \\mathcal{P}(X^{n})\\wedge |X|=k}\\frac{1}{k}\\bigg(\\sum_{i\\in X}[y_{i}\\neq f_{X^{n}\\setminus X}( x_{i})]\\bigg),$$\n",
    "where $\\mathcal{P}(X^{n})$ is the set of all subsets of $X^{n}$. For all $i=1,2\\dots,n$ denote the label of $m$-th closest neighbor of $x_{i}$ in $X^{n}\\setminus \\{x_{i}\\}$ by $y_{i}^{m}$. Show that \n",
    "$$L_{k}OCV=\\sum_{m=1}^{k}\\underbrace{\\frac{1}{n}\\sum_{i=1}^{n}[y_{i}\\neq y_{i}^{m}]}_{K_{m}(X^{n})}\\frac{C_{n-1-m}^{n-k-1}}{C_{n-1}^{k-1}},$$\n",
    "where $K_{m}(X^{n})$ is called the compactness profile of $X^{n}$, i.e. the fraction of objects whose $m$-th nearest neighbor has different label. For convenience assume that all the pairwise distances between objects are different.\n",
    "### Your solution:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In **leave-k-out cross-validation** for $k \\in i = 1, . . . , n$ we train the model on every data except held out data $\\in k$.\n",
    "- Thus, we need to calculate the number of subsets $X^n$ with cardinality $k$ $\\implies$ we need to pick $n-k-1$ objects from set of $n-m-1$ objects, which takes $C^{n-k-1}_{n-m-1}$ combinations.\n",
    "- For every $ x_i \\in X^n, i = 1,...,n $ we choose $k$ elements,which takes  $C_{n}^{k}$ $\\implies$\n",
    "\n",
    "\n",
    "$$L_{k}OCV=\\sum\\limits_{X\\subset \\mathcal{P}(X^{n})\\wedge |X|=k}\\frac{1}{k}\\bigg(\\sum_{i\\in X}[y_{i}\\neq f_{X^{n}\\setminus X}( x_{i})]\\bigg) = \\sum\\limits_{i=1}^n  \\sum\\limits_{m=1}^k \\frac{1}{k} \\frac{1}{C_{n}^{k}}C_{n-m-1}^{n-k-1} [ y_i \\neq y_i^m] $$ \n",
    "\n",
    " $$ \\frac{1}{k C_{n}^{k}} = \\frac{k!(n-k)!}{kn!}=\\frac{(k-1)!(n-k)!}{n(n-1)!} =\\frac{1}{n C_{k-1}^{n-1}}\\implies$$\n",
    " \n",
    " \n",
    "\n",
    "$$L_{k}OCV = \\sum\\limits_{i=1}^n  \\sum\\limits_{m=1}^k \\frac{1}{n}\\frac{1}{C_{k-1}^{n-1}}C_{n-m-1}^{n-k-1} [ y_i \\neq y_i^m] = \\sum_{m=1}^{k}\\underbrace{\\frac{1}{n}\\sum_{i=1}^{n}[y_{i}\\neq y_{i}^{m}]}_{K_{m}(X^{n})}\\frac{C_{n-1-m}^{n-k-1}}{C_{n-1}^{k-1}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5. Bootstrap (1 point)\n",
    "Let the subsample $\\hat{X}^{n}$ of size $n$ be generated from sample $X^{n}=\\{\\boldsymbol{x}_{1},\\dots\\boldsymbol{x}_{n}\\}$ by bootstrap procedure. Find the probability that object $x_{i}$ is not present in $\\hat{X}^{n}$ and compute the limit of this probability for $n\\rightarrow\\infty$.\n",
    "### Your solution:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For every $x_i$ probability of chosing is equal in each bootstrap procedure  $P(x_i \\in \\hat{X}^n_i) = \\frac{1}{n}$\n",
    "$$\n",
    "P(x_i \\notin \\hat{X}^n)  \n",
    "= (P(x_i \\notin \\hat{X}^n_i))^n = (1 - P(x_i \\in \\hat{X}^n_i))^n  \n",
    "= (1 - \\cfrac{1}{n})^n \n",
    "$$\n",
    "$$\\lim\\limits_{n \\rightarrow \\infty}(1 + \\frac{\\alpha}{n})^n = e^{\\alpha} \\implies$$\n",
    "$$\n",
    "  \\lim\\limits_{n \\rightarrow \\infty}P(x_i \\notin \\hat{X}^n) = e^{-1} \\approx\t0.37\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6. Decision Tree Leaves (1+1=2 points)\n",
    "\n",
    "Consider a leaf of a binary decision tree which consists of object-label pairs $(x_{1},y_{1}),\\dots,(x_{n},y_{n})$. The prediction $\\hat{y}$ of this leaf is defined to minimize the loss on the training sample.\n",
    "\n",
    "* For a classification tree for K classes ($y_{i}\\in\\{1,2,\\dots,K\\}$) and zero-one loss $L(y,\\hat{y})=[y\\neq \\hat{y}]$, find the optimal prediction in the leaf.\n",
    "\n",
    "* For a regression tree ($y_{i}\\in\\mathbb{R}$) and squared percentage error loss $L(y,\\hat{y})=\\frac{(y-\\hat{y})^{2}}{y^2}$ find the optimal prediction in the leaf.\n",
    "\n",
    "\n",
    "\n",
    "### Your solution:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1) $L(y,\\hat{y})=[y\\neq \\hat{y}]$ is not a convex function, thus it cannot be optimized by ordinary means. In this case the optimization problem is $\\frac{1}{n}\\sum_{i=1}^{n}[y_{i}\\neq\\hat{y}] \\to min$ or $\\frac{1}{n}\\sum_{i=1}^{n}[y_{i}=\\hat{y}] \\to max$. Intuitively, the optimal prediction that solves this optimization problem is the most frequent label:\n",
    "$$\\hat{y}=\\underset{k=1,2,\\dots,K}{\\operatorname{argmax}}\\frac{1}{n}\\sum_{i=1}^{n}[y_{i}=k].$$\n",
    "\n",
    "or\n",
    "\n",
    "$$\\hat{y}=\\underset{k=1,2,\\dots,K}{\\operatorname{argmin}}\\frac{1}{n}\\sum_{i=1}^{n}[y_{i}\\neq k].$$\n",
    "\n",
    "- 2)$L(y,\\hat{y})=\\frac{(y-\\hat{y})^{2}}{y^2}$ In this case the optimization problem is $\\frac{1}{n}\\sum_{i=1}^{n}\\frac{(y_{i}-\\hat{y})^{2}}{y_{i}^2} \\to min$\n",
    "$$\\frac{1}{n}\\sum_{i=1}^{n}\\frac{(y_{i}-\\hat{y})^{2}}{y_{i}^2} = \n",
    "\\frac{1}{n}\\sum_{i=1}^{n}\\frac{(y_{i}^2-2\\hat{y}y_i +\\hat{y^2})}{y_{i}^2}=\n",
    "\\frac{1}{n}\\sum_{i=1}^{n}\\bigg[1-\\frac{2\\hat{y}}{y_i} +\\frac{\\hat{y^2}}{y_i^2}\\bigg]=\n",
    "1-\\frac{2\\hat{y}}{n} \\sum_{i=1}^{n} \\frac{1}{y_i}+\\frac{\\hat{y}^2}{n} \\sum_{i=1}^{n}\\frac{1}{y_i^2} \\to min\n",
    "$$\n",
    "\n",
    "$$\\nabla L = \\frac{2\\hat{y}}{n} \\sum_{i=1}^{n}\\frac{1}{y_i^2} + \\frac{\\hat{y}^2}{n} \\sum_{i=1}^{n}\\frac{-2}{y_i^3}=0$$\n",
    "\n",
    "$$2 \\sum_{i=1}^{n} \\frac{1}{y_i^2} + \\hat{y} \\sum_{i=1}^{n} \\frac{-2}{y_i^3}=0  \\implies$$\n",
    "\n",
    "$$\\hat{y} = \\frac{\\sum_{i=1}^{n} y_i^{-2}}{\\sum_{i=1}^{n} y_i^{-3}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 7. Classification (1 point)\n",
    "Let objects $\\boldsymbol{x}_{1},\\dots,\\boldsymbol{x}_{n}$ have binary labels $y_{1}, y_{2},\\dots,y_{n}\\in\\{0,1\\}$. Let the classifier $f$ assign to objects probabilities of being from class $1$. Without loss of generality assume that $f(\\boldsymbol{x_{i}})<f(\\boldsymbol{x_{j}})$ for all $i<j$. Denote the number of objects of $X^{n}$ from class $1$ by $n_{1}=\\sum_{i=1}^{n}[y_{i}=1]$. Show that \n",
    "$$S_{\\text{ROC}}(f,X^{n})=\\frac{1}{n_{1}(n-n_{1})}\\sum_{i<j}[y_{i}<y_{j}]$$\n",
    "where $S_{\\text{ROC}}(f,X^{n})$ is the Area Under ROC of classifier $f$.\n",
    "### Your solution:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "asd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 8. Kernels (1+1=2 points)\n",
    "Kernel $K(x,y)$ corresponds to dot product of feature maps $\\varphi$ and therefore $K(x,y) = \\langle \\varphi(x), \\varphi(y) \\rangle$. Derive functions $\\varphi$ for the following kernels:\n",
    "* $K(x,y)=\\langle x, y \\rangle ^ d$;\n",
    "* $K(x,y)= \\left(c + \\langle x, y \\rangle \\right)^ d$  with $c\\geq 0$;\n",
    "\n",
    "### Your solution:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1) Dot product can be represented as:\n",
    "$$\\langle x, y \\rangle ^ d = (x^{T}y)^{d} = (x_1y_1+x_2y_2+ \\dots x_n y_n)^d$$\n",
    "\n",
    "https://en.wikipedia.org/wiki/Multinomial_theorem $\\implies$\n",
    "\n",
    "$$(x_1 + x_2  + \\cdots + x_m)^n = \\sum_{k_1+k_2+\\cdots+k_m=n} {n \\choose k_1, k_2, \\ldots, k_m}\n",
    "  \\prod_{t=1}^m x_t^{k_t}\\,,$$\n",
    "where:$$ {n \\choose k_1, k_2, \\ldots, k_m}\n",
    " = \\frac{n!}{k_1!\\, k_2! \\cdots k_m!}$$\n",
    " \n",
    "$$ \\implies (x_1y_1+x_2y_2+ \\dots x_n y_n)^d = \\sum_{k_1+k_2+\\cdots+k_n=d} {d \\choose k_1, k_2, \\ldots, k_n}\n",
    "  \\prod_{t=1}^n (x_t^{k_t})y_t^{k_t}\\, \\implies$$\n",
    "  \n",
    "$$\\varphi(x) = \\bigg[\\sum_{k_1+k_2+\\cdots+k_n=d} \\sqrt{{d \\choose k_1, k_2, \\ldots, k_n}}\\prod_{t=1}^n x_t^{k_t}\\bigg]_{i}^{nd}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 2) Using binomial theorem:$ \\quad (a+b)^d = \\sum_{k=0}^d \\binom{d}{k} a^{d - k} b^k$\n",
    "$$K(x,y) = \\left(\\sum_{i=1}^n x_i y_i + c\\right)^d = \\sum_{k=0}^d \\binom{d}{k} (\\sum_{i=1}^n x_i y_i)^{d - k} c^k\n",
    "$$\n",
    "\n",
    "$$\\varphi(x) = \\bigg[\\sum_{k=0}^d\\sqrt{\\binom{d}{k}} (\\sum_{i=1}^n x_i)^{d - k} c^k\\bigg]_{i}^{nd}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 9. Kernel Methods (1 point)\n",
    "Prove that Gaussian Kernel $K(x,x')=\\exp(-\\|x-x'\\|^{2})$ is positive definite. You do not need to prove that the linear kernel is positive definite.\n",
    "### Your solution:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\|x - x'\\|^2 = \\|x\\|^2 -  2 x^\\top x' +  \\|x'\\|^2 \\implies$\n",
    "$K(x,x')=\\exp(-\\|x-x'\\|^{2}) = \\exp(-\\|x\\|^2 + 2 x^\\top x' - \\|x'\\|^2) = \\exp(-\\|x\\|^2)\\exp(2x^\\top x') \\exp(-\\|x'\\|^2)$\n",
    "\n",
    "The function $\\exp(-\\|x\\|^2)>0$ $\\implies$\n",
    "\n",
    "To prove that $K(x,x')$ is PD we need to prove that $\\exp(2x^\\top x')$ is PD.\n",
    "\n",
    "- Using Taylor series:\n",
    "$$\\exp(2x^\\top x') = 1 + 2 x^\\top x' + \\frac{2^2}{2!} (x^\\top x')^2 + \\dots = \\sum\\limits_{i=0}^{\\infty} \\frac{(2x^\\top x')^i}{i!} $$\n",
    "\n",
    "- We can see that assosiated kernel matrix $K=(K(x_i,x_j))_{i,j=1}^N$ for linear kernel $x^{T}x'$  is semi-positive definite $\\implies exp(2x^{T}x')$ is also SPD $\\implies K(x,x')$ is PD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 10. Support Vector Machine (1 point)\n",
    "Show that for two-class SVM classifier the following upper bound on accuracy leave-one-out-cross-validation estimate holds true:\n",
    "$$L_{1}OCV=\\frac{1}{n}\\sum_{i=1}^{n}[y_{i}=f_{i}(x_{i})]\\leq \\frac{|SV|}{n},$$\n",
    "where for all $i=1,\\dots,n$ $f_{i}(x_{i})$ is SVM fitted on the entire data without observation $(x_{i},y_{i})$ and $|SV|$ is the number of support vectors of SVM fit on the entire data.\n",
    "### Your solution:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- First of all let's note that:\n",
    "$$L_{1}OCV=\\frac{1}{n}\\sum_{i=1}^{n}[y_{i}=f_{i}(x_{i})]\\leq 1, \\quad (1)$$\n",
    "\n",
    "\n",
    "\n",
    "- From the SVM lecture(24th slide) we got the Lagrangian:\n",
    "$$L = \\sum_{i=1}^{n} \\alpha_{i} - \\frac{1}{2} \\sum_{i,j}\\alpha_{i}\\alpha_{j}y_i y_j(x_i \\cdot x_{j}^{T})$$\n",
    "- Maximizing the function:\n",
    "$$\\max_{\\alpha} \\sum_{i=1}^{n} \\alpha_i - \\frac{1}{2} \\sum_{i=1}^n \\sum_{j=1}^n y_i y_j \\alpha_i \\alpha_j (x_i \\cdot x_{j}^{T})$$\n",
    "subject to $\\alpha_i \\geq 0$ and $\\sum_{i=0}^{n} \\alpha_i y_i=0, i \\in[1,n]$\n",
    "- By forming the Lagrangian:\n",
    "$$\\mathcal{L}(\\alpha, \\beta, \\lambda) = \\sum_{i=1}^n \\alpha_i - \\frac{1}{2} \\sum_{i=1}^n \\sum_{j=1}^n y_i y_j \\alpha_i \\alpha_j (x_i \\cdot x_{j}^{T}) - \\sum_{i=1}^n \\beta_i \\alpha_i - \\lambda \\sum_{i=1}^n \\alpha_i y_i$$\n",
    "- and taking the derivative with respect to $\\alpha_i$, then setting it to zero, we get:\n",
    "$$\\alpha_i = \\frac{1}{y_i} \\left[ \\frac{1}{y_i} - \\frac{1}{n} \\sum_{i=j}^n \\frac{1}{y_j}\\right]$$\n",
    "- Since $y_i=±1$ we can rewrite:\n",
    "$$\\alpha_i = \\frac{1}{y_i} \\left[ \\frac{1}{y_i} - \\frac{1}{n} \\sum_{j=1}^n \\frac{1}{y_j}\\right] = y_i \\left[y_i - \\frac{1}{n} \\sum_{j=1}^n y_j\\right] = 1 - y_{i}\\frac{N^{+}-N^{-}}{N}$$\n",
    "\n",
    "where$N^{+}$, $N^{-}$ are number samples in positive and negative class respectively.\n",
    "Also, $\\sum_{i}^{n} \\alpha_{i}y_{i} = 0$ and $\\alpha_i > 0$ meaning that all the vectors are support vectors: $$\\|SV\\|=n$$\n",
    "- Thus, we get:\n",
    "$$L_{1}OCV=\\frac{1}{n}\\sum_{i=1}^{n}[y_{i}=f_{i}(x_{i})]\\leq \\frac{|SV|}{n}=\\frac{n}{n}=1$$\n",
    "$\\implies$ $(1)$ holds true."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The end."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
